{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe17ecb4",
   "metadata": {},
   "source": [
    "# Blog Posts Data Ingestion Pipeline\n",
    "\n",
    "This notebook implements a data ingestion pipeline to extract blog posts from the aymenfurter.ch website, process the information using AI, and store it in a structured format. The pipeline leverages Azure OpenAI for intelligent data extraction and the Microsoft Copilot Platform (MCP) for web browsing capabilities.\n",
    "\n",
    "### 1. Reading Secrets from Azure Key Vault\n",
    "This section retrieves the necessary credentials from Azure Key Vault to authenticate with the required services.\n",
    "- **MCP Subscription Key**: To access the Microsoft Copilot Platform (MCP) server for web crawling.\n",
    "- **Azure OpenAI API Key**: To authenticate with the Azure OpenAI service for data extraction.\n",
    "\n",
    "This notebook is designed for environments like Microsoft Fabric, where `notebookutils` provides seamless access to Key Vault secrets. Ensure the Key Vault is populated and the notebook's execution identity has appropriate permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_vault_uri = \"https://yourkeyvault.vault.azure.net/\"\n",
    "MCP_SUBSCRIPTION_KEY = notebookutils.credentials.getSecret(key_vault_uri, \"mcp-key\")\n",
    "AZURE_OPENAI_API_KEY = notebookutils.credentials.getSecret(key_vault_uri, \"openai-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff50b0",
   "metadata": {},
   "source": [
    "### 2. Service and Tool Configuration\n",
    "This section configures the clients and tools required for the data extraction process.\n",
    "\n",
    "- **Azure OpenAI Client**: An `AzureOpenAI` client is initialized with the endpoint, API key, and API version. This client will be used to make calls to the GPT model.\n",
    "- **MCP Tool**: A tool definition for the Model Context Protocol (MCP) is created. This tool enables the AI model to perform web browsing actions through Playwright, such as navigating to URLs to fetch blog data. The tool is configured with the MCP server URL and the necessary authentication headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55cf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "AZURE_OPENAI_DEPLOYMENT = \"gpt-4.1\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://your-endpoint.openai.azure.com/\"\n",
    "API_VERSION = \"preview\"\n",
    "\n",
    "MCP_SERVER_URL = \"https://mcp-apim-your-endpoint.azure-api.net/mcp\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    base_url=f\"{AZURE_OPENAI_ENDPOINT}openai/v1/\",\n",
    "    api_version=\"preview\"\n",
    ")\n",
    "\n",
    "mcp_tool = {\n",
    "    \"type\": \"mcp\",\n",
    "    \"server_label\": \"mcp-server\",\n",
    "    \"server_url\": MCP_SERVER_URL,\n",
    "    \"headers\": {\"Ocp-Apim-Subscription-Key\": MCP_SUBSCRIPTION_KEY},\n",
    "    \"require_approval\": \"never\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1d397",
   "metadata": {},
   "source": [
    "### 3. Defining Data Structures with Pydantic\n",
    "To ensure the extracted data is structured and validated, we define Pydantic models. These models serve as schemas for the expected output from the AI model.\n",
    "\n",
    "- **`BlogUrlResult`**: Defines the structure for the initial response containing a list of blog post URLs.\n",
    "- **`BlogDetails`**: A comprehensive model that defines the schema for all the details of a single blog post, including title, content, posting date, and more.\n",
    "- **`ExtractionResult`**: A wrapper model to hold the result of a single blog detail extraction, including the status and the `BlogDetails` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac893830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class BlogUrlResult(BaseModel):\n",
    "    blog_urls: List[str] = Field(..., description=\"List of blog post URLs extracted from the page\")\n",
    "    total_found: int = Field(..., description=\"Total number of blog post URLs found\")\n",
    "    extraction_successful: bool = Field(..., description=\"Whether the URL extraction was successful\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if extraction failed\")\n",
    "\n",
    "class BlogDetails(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the blog post\")\n",
    "    url: str = Field(..., description=\"The URL of the blog post\")\n",
    "    content: str = Field(..., description=\"The full content of the blog post\")\n",
    "    posting_date: Optional[str] = Field(None, description=\"The date the blog post was published\")\n",
    "    duration_minutes: Optional[int] = Field(None, description=\"The estimated reading time in minutes\")\n",
    "    topics: List[str] = Field(default_factory=list, description=\"A list of topics or tags associated with the blog post\")\n",
    "    technologies_used: List[str] = Field(default_factory=list, description=\"A list of technologies mentioned or used in the blog post\")\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    extraction_successful: bool = Field(..., description=\"Whether extraction was successful\")\n",
    "    error_message: Optional[str] = None\n",
    "    blog_details: Optional[BlogDetails] = None\n",
    "    url_processed: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e6766",
   "metadata": {},
   "source": [
    "### 4. AI-Powered Extraction Functions\n",
    "These functions orchestrate the interaction with the Azure OpenAI service to perform the extraction tasks. They use the `client.responses.parse` method, which simplifies making a request to the model and parsing the structured response directly into the Pydantic models defined earlier.\n",
    "\n",
    "- **`extract_blog_urls()`**: Instructs the AI model to navigate to the `aymenfurter.ch` articles page and extract all available blog post URLs.\n",
    "- **`extract_blog_details(blog_url)`**: Takes a blog post URL, instructs the model to navigate to it, and extracts detailed information according to the `BlogDetails` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blog_urls():\n",
    "    prompt = \"Navigate to: https://aymenfurter.ch/articles/ and extract all blog post URLs.\"\n",
    "    print(\"STEP 1: Extracting blog URLs\")\n",
    "    return client.responses.parse(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a scraper that extracts blog post URLs from a webpage.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        tools=[mcp_tool],\n",
    "        text_format=BlogUrlResult\n",
    "    ).output_parsed\n",
    "\n",
    "def extract_blog_details(blog_url):\n",
    "    detail_prompt = f\"\"\"Navigate to: {blog_url}\n",
    "Extract the blog post details. This includes the full content, posting date, duration in minutes, topics, and any technologies mentioned.\n",
    "The URL of the blog post is {blog_url}.\"\"\"\n",
    "    return client.responses.parse(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract blog post data clearly and return structured output.\"},\n",
    "            {\"role\": \"user\", \"content\": detail_prompt}\n",
    "        ],\n",
    "        tools=[mcp_tool],\n",
    "        text_format=ExtractionResult\n",
    "    ).output_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08406a1c",
   "metadata": {},
   "source": [
    "### 5. Spark DataFrame Schema and Record Preparation\n",
    "This section defines helper functions to prepare the extracted data for storage in a Spark DataFrame.\n",
    "\n",
    "- **`spark_schema_from_model(model_cls)`**: Dynamically generates a Spark `StructType` schema from a Pydantic model. It also adds metadata fields like `extraction_status`, `error_message`, and `extraction_timestamp`.\n",
    "- **`build_record(...)`**: Transforms an extracted `BlogDetails` object into a dictionary that conforms to the Spark schema. It handles the serialization of list-based fields (like topics and technologies) into JSON strings for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from typing import get_origin, get_args\n",
    "\n",
    "def spark_schema_from_model(model_cls):\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        str: StringType(),\n",
    "        int: IntegerType()\n",
    "    }\n",
    "    for name, model_field in model_cls.model_fields.items():\n",
    "        field_type = model_field.annotation\n",
    "        if get_origin(field_type) is type(Optional[str]):\n",
    "            field_type = get_args(field_type)[0]\n",
    "        if get_origin(field_type) is list:\n",
    "            spark_type = StringType()\n",
    "        else:\n",
    "            spark_type = type_mapping.get(field_type, StringType())\n",
    "        fields.append(StructField(name, spark_type, True))\n",
    "    \n",
    "    extras = [\n",
    "        (\"extraction_status\", StringType()),\n",
    "        (\"error_message\", StringType()),\n",
    "        (\"extraction_timestamp\", TimestampType())\n",
    "    ]\n",
    "    for name, dtype in extras:\n",
    "        fields.append(StructField(name, dtype, True))\n",
    "    return StructType(fields)\n",
    "\n",
    "def build_record(extraction_timestamp, blog_details: BlogDetails):\n",
    "    base = blog_details.model_dump(exclude_none=True)\n",
    "    record = {}\n",
    "    for k, v in base.items():\n",
    "        if isinstance(v, list):\n",
    "            record[k] = json.dumps(v)\n",
    "        else:\n",
    "            record[k] = v\n",
    "    record[\"extraction_status\"] = \"success\"\n",
    "    record[\"error_message\"] = None\n",
    "    record[\"extraction_timestamp\"] = extraction_timestamp\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072acc0",
   "metadata": {},
   "source": [
    "### 6. Main Execution: Extract, Transform, and Load\n",
    "This is the main execution block that runs the end-to-end data ingestion pipeline.\n",
    "\n",
    "1.  **Extract Blog URLs**: Calls `extract_blog_urls()` to get a list of all blog postings.\n",
    "2.  **Extract Blog Details**: Iterates through the list of URLs, calling `extract_blog_details()` for each one to get detailed information.\n",
    "3.  **Create DataFrame**: The extracted and transformed records are used to create a Spark DataFrame.\n",
    "4.  **Save to Table**: The DataFrame is written to a Delta table, overwriting any existing data. This makes the blog posts data available for querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af832bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"STEP 1: Extracting blog URLs\")\n",
    "urls_result = extract_blog_urls()\n",
    "print(f\"STEP 1 RESULTS: Found {urls_result.total_found} blog URLs\")\n",
    "\n",
    "if urls_result.extraction_successful and urls_result.blog_urls:\n",
    "    extraction_timestamp = datetime.now()\n",
    "    to_process = urls_result.blog_urls\n",
    "    records = []\n",
    "    for url in to_process:\n",
    "        print(f\"STEP 2: Extracting blog details from {url}\")\n",
    "        result = extract_blog_details(url)\n",
    "        blog = result.blog_details\n",
    "        if blog:\n",
    "            print(f\"Success: {blog.title}\")\n",
    "            records.append(build_record(extraction_timestamp, blog))\n",
    "\n",
    "    schema = spark_schema_from_model(BlogDetails)\n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "    table_name = \"aymenfurter_blogs\"\n",
    "\n",
    "    df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "    success_count = df.filter(F.col(\"extraction_status\") == \"success\").count()\n",
    "    total = len(to_process)\n",
    "    print(\"Ingestion Complete\")\n",
    "    print(f\"Overwrote table {table_name}\")\n",
    "    print(f\"Total processed {total} Success {success_count}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
